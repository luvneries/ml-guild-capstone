{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used for exploratory analysis, design functions for WALS module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import modules\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#curl -O 'http://files.grouplens.org/datasets/movielens/ml-100k.zip'\n",
    "#unzip ml-100k.zip\n",
    "#cp ml-100k/u.data data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>ratings</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  ratings  timestamp\n",
       "0      196      242        3  881250949\n",
       "1      186      302        3  891717742\n",
       "2       22      377        1  878887116\n",
       "3      244       51        2  880606923\n",
       "4      166      346        1  886397596"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/u.data', sep='\\t', names=['user_id', 'item_id', 'ratings', 'timestamp'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 4 columns):\n",
      "user_id      100000 non-null int64\n",
      "item_id      100000 non-null int64\n",
      "ratings      100000 non-null int64\n",
      "timestamp    100000 non-null int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 3.1 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>ratings</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1.000000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>462.48475</td>\n",
       "      <td>425.530130</td>\n",
       "      <td>3.529860</td>\n",
       "      <td>8.835289e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>266.61442</td>\n",
       "      <td>330.798356</td>\n",
       "      <td>1.125674</td>\n",
       "      <td>5.343856e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.747247e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>254.00000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.794487e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>447.00000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.828269e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>682.00000</td>\n",
       "      <td>631.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.882600e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>943.00000</td>\n",
       "      <td>1682.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.932866e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id        item_id        ratings     timestamp\n",
       "count  100000.00000  100000.000000  100000.000000  1.000000e+05\n",
       "mean      462.48475     425.530130       3.529860  8.835289e+08\n",
       "std       266.61442     330.798356       1.125674  5.343856e+06\n",
       "min         1.00000       1.000000       1.000000  8.747247e+08\n",
       "25%       254.00000     175.000000       3.000000  8.794487e+08\n",
       "50%       447.00000     322.000000       4.000000  8.828269e+08\n",
       "75%       682.00000     631.000000       4.000000  8.882600e+08\n",
       "max       943.00000    1682.000000       5.000000  8.932866e+08"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe info\n",
    "\n",
    "df.info()\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 11:33:32.758182 4535608768 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/site-packages/tensorflow_transform/analyzers.py:948: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0714 11:33:32.768829 4535608768 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/site-packages/tensorflow_transform/analyzers.py:994: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id='facets-iframe' width=\"100%\" height=\"500px\"></iframe>\n",
       "        <script>\n",
       "        facets_iframe = document.getElementById('facets-iframe');\n",
       "        facets_html = '<link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/master/facets-dist/facets-jupyter.html\"><facets-overview proto-input=\"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\"></facets-overview>';\n",
       "        facets_iframe.contentWindow.document.write(facets_html);\n",
       "         facets_iframe.id = \"\";\n",
       "         setTimeout(() => {\n",
       "           facets_iframe.setAttribute('height', facets_iframe.contentWindow.document.body.offsetHeight + 'px')\n",
       "         }, 1500)\n",
       "         </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate statistics\n",
    "train_stats = tfdv.generate_statistics_from_dataframe(df)\n",
    "tfdv.visualize_statistics(train_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data from ratings dataframe or web views\n",
    "def create_test_and_train_sets(args, input_file, data_type='ratings'):\n",
    "    if data_type == 'ratings':\n",
    "        return _ratings_train_and_test(args['headers'], args['delimiter'],\n",
    "                                   input_file)\n",
    "    elif data_type == 'web_views':\n",
    "        return _page_views_train_and_test(input_file)\n",
    "    else:\n",
    "        raise ValueError('data_type arg value %s not supported.' % data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test dataframe\n",
    "def _ratings_train_and_test(use_headers, delimiter, input_file):\n",
    "    headers = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "    header_row = 0 if use_headers else None\n",
    "    ratings_df = pd.read_csv(input_file,\n",
    "                           sep=delimiter,\n",
    "                           names=headers,\n",
    "                           header=header_row,\n",
    "                           dtype={\n",
    "                               'user_id': np.int32,\n",
    "                               'item_id': np.int32,\n",
    "                               'rating': np.float32,\n",
    "                               'timestamp': np.int32,\n",
    "                           })\n",
    "\n",
    "    np_users = ratings_df.user_id.values\n",
    "    np_items = ratings_df.item_id.values\n",
    "    unique_users = np.unique(np_users)\n",
    "    unique_items = np.unique(np_items)\n",
    "\n",
    "    n_users = unique_users.shape[0]\n",
    "    n_items = unique_items.shape[0]\n",
    "\n",
    "    # make indexes for users and items if necessary\n",
    "    max_user = unique_users[-1]\n",
    "    max_item = unique_items[-1]\n",
    "    if n_users != max_user or n_items != max_item:\n",
    "        # make an array of 0-indexed unique user ids corresponding to the dataset\n",
    "        # stack of user ids\n",
    "        z = np.zeros(max_user+1, dtype=int)\n",
    "        z[unique_users] = np.arange(n_users)\n",
    "        u_r = z[np_users]\n",
    "\n",
    "        # make an array of 0-indexed unique item ids corresponding to the dataset\n",
    "        # stack of item ids\n",
    "        z = np.zeros(max_item+1, dtype=int)\n",
    "        z[unique_items] = np.arange(n_items)\n",
    "        i_r = z[np_items]\n",
    "\n",
    "        # construct the ratings set from the three stacks\n",
    "        np_ratings = ratings_df.rating.values\n",
    "        ratings = np.zeros((np_ratings.shape[0], 3), dtype=object)\n",
    "        ratings[:, 0] = u_r\n",
    "        ratings[:, 1] = i_r\n",
    "        ratings[:, 2] = np_ratings\n",
    "    else:\n",
    "        ratings = ratings_df[['user_id', 'item_id', 'rating']].values\n",
    "        # deal with 1-based user indices\n",
    "        ratings[:, 0] -= 1\n",
    "        ratings[:, 1] -= 1\n",
    "\n",
    "    tr_sparse, test_sparse = _create_sparse_train_and_test(ratings,\n",
    "                                                         n_users, n_items)\n",
    "\n",
    "    return ratings[:, 0], ratings[:, 1], tr_sparse, test_sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test and train into coo_matrix to process efficiently\n",
    "TEST_SET_RATIO = 10\n",
    "def _create_sparse_train_and_test(ratings, n_users, n_items):\n",
    "\n",
    "    # pick a random test set of entries, sorted ascending\n",
    "    test_set_size = len(ratings) // TEST_SET_RATIO\n",
    "    test_set_idx = np.random.choice(range(len(ratings)),\n",
    "                                  size=test_set_size, replace=False)\n",
    "    test_set_idx = sorted(test_set_idx)\n",
    "\n",
    "    # split ratings into train and test sets\n",
    "    ts_ratings = ratings[test_set_idx]\n",
    "    tr_ratings = np.delete(ratings, test_set_idx, axis=0)\n",
    "\n",
    "    # create training and test matrices as coo_matrix's\n",
    "    u_tr, i_tr, r_tr = zip(*tr_ratings)\n",
    "    tr_sparse = coo_matrix((r_tr, (u_tr, i_tr)), shape=(n_users, n_items))\n",
    "\n",
    "    u_ts, i_ts, r_ts = zip(*ts_ratings)\n",
    "    test_sparse = coo_matrix((r_ts, (u_ts, i_ts)), shape=(n_users, n_items))\n",
    "\n",
    "    return tr_sparse, test_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use coo_matrix and convert them into sparse Tensor\n",
    "# Instantiate WALS factorization model\n",
    "# Generate row and column factors\n",
    "\n",
    "from tensorflow.contrib.factorization.python.ops import factorization_ops\n",
    "\n",
    "LOG_RATINGS = 0\n",
    "LINEAR_RATINGS = 1\n",
    "LINEAR_OBS_W = 100.0\n",
    "\n",
    "def wals_model(data, dim, reg, unobs, weights=False,\n",
    "               wt_type=LINEAR_RATINGS, feature_wt_exp=None,\n",
    "               obs_wt=LINEAR_OBS_W):\n",
    "    row_wts = None\n",
    "    col_wts = None\n",
    "\n",
    "    num_rows = data.shape[0]\n",
    "    num_cols = data.shape[1]\n",
    "\n",
    "    if weights:\n",
    "        assert feature_wt_exp is not None\n",
    "        row_wts = np.ones(num_rows)\n",
    "        col_wts = make_wts(data, wt_type, obs_wt, feature_wt_exp, 0)\n",
    "\n",
    "    row_factor = None\n",
    "    col_factor = None\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        input_tensor = tf.SparseTensor(indices=list(zip(data.row, data.col)),\n",
    "                                   values=(data.data).astype(np.float32),\n",
    "                                   dense_shape=data.shape)\n",
    "\n",
    "        model = factorization_ops.WALSModel(num_rows, num_cols, dim,\n",
    "                                        unobserved_weight=unobs,\n",
    "                                        regularization=reg,\n",
    "                                        row_weights=row_wts,\n",
    "                                        col_weights=col_wts)\n",
    "\n",
    "        # retrieve the row and column factors\n",
    "        row_factor = model.row_factors[0]\n",
    "        col_factor = model.col_factors[0]\n",
    "\n",
    "    return input_tensor, row_factor, col_factor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weights for WALS model\n",
    "\n",
    "def make_wts(data, wt_type, obs_wt, feature_wt_exp, axis):\n",
    "\n",
    "    # recipricol of sum of number of items across rows (if axis is 0)\n",
    "    frac = np.array(1.0/(data > 0.0).sum(axis))\n",
    "\n",
    "    # filter any invalid entries\n",
    "    frac[np.ma.masked_invalid(frac).mask] = 0.0\n",
    "\n",
    "    # normalize weights according to assumed distribution of ratings\n",
    "    if wt_type == LOG_RATINGS:\n",
    "        wts = np.array(np.power(frac, feature_wt_exp)).flatten()\n",
    "    else:\n",
    "        wts = np.array(obs_wt * frac).flatten()\n",
    "\n",
    "    # check again for any numerically unstable entries\n",
    "    assert np.isfinite(wts).sum() == wts.shape[0]\n",
    "    return wts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build session\n",
    "\n",
    "def build_session(model, input_tensor, num_iterations):\n",
    "\n",
    "    sess = tf.Session(graph=input_tensor.graph)\n",
    "\n",
    "    with input_tensor.graph.as_default(): \n",
    "        row_update_op = model.update_row_factors(sp_input=input_tensor)[1]\n",
    "        col_update_op = model.update_col_factors(sp_input=input_tensor)[1]\n",
    "\n",
    "        sess.run(model.initialize_op)\n",
    "        sess.run(model.worker_init)\n",
    "        for _ in range(num_iterations):\n",
    "            sess.run(model.row_update_prep_gramian_op)\n",
    "            sess.run(model.initialize_row_update_op)\n",
    "            sess.run(row_update_op)\n",
    "            sess.run(model.col_update_prep_gramian_op)\n",
    "            sess.run(model.initialize_col_update_op)\n",
    "            sess.run(col_update_op)\n",
    "\n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train WALS model\n",
    "\n",
    "def train_model(args, tr_sparse):\n",
    "\n",
    "    dim = args['latent_factors']\n",
    "    num_iters = args['num_iters']\n",
    "    reg = args['regularization']\n",
    "    unobs = args['unobs_weight']\n",
    "    wt_type = args['wt_type']\n",
    "    feature_wt_exp = args['feature_wt_exp']\n",
    "    obs_wt = args['feature_wt_factor']\n",
    "\n",
    "    tf.logging.info('Train Start: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "\n",
    "    # generate model\n",
    "    input_tensor, row_factor, col_factor, model = wals_model(tr_sparse,\n",
    "                                                                dim,\n",
    "                                                                reg,\n",
    "                                                                unobs,\n",
    "                                                                args['weights'],\n",
    "                                                                wt_type,\n",
    "                                                                feature_wt_exp,\n",
    "                                                                obs_wt)\n",
    "\n",
    "    # factorize matrix\n",
    "    session = build_session(model, input_tensor, num_iters)\n",
    "\n",
    "    tf.logging.info('Train Finish: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "\n",
    "    # evaluate output factor matrices\n",
    "    output_row = row_factor.eval(session=session)\n",
    "    output_col = col_factor.eval(session=session)\n",
    "\n",
    "    # close the training session now that we've evaluated the output\n",
    "    session.close()\n",
    "\n",
    "    return output_row, output_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"Parse job arguments.\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # required input arguments\n",
    "    parser.add_argument(\n",
    "      '--train-files',\n",
    "      help='GCS or local paths to training data',\n",
    "      nargs='+',\n",
    "      required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "      '--job-dir',\n",
    "      help='GCS location to write checkpoints and export models',\n",
    "      required=False\n",
    "    )\n",
    "\n",
    "    # hyper params for model\n",
    "    parser.add_argument(\n",
    "      '--latent_factors',\n",
    "      type=int,\n",
    "      help='Number of latent factors',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "      '--num_iters',\n",
    "      type=int,\n",
    "      help='Number of iterations for alternating least squares factorization',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "      '--regularization',\n",
    "      type=float,\n",
    "      help='L2 regularization factor',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "      '--unobs_weight',\n",
    "      type=float,\n",
    "      help='Weight for unobserved values',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "      '--wt_type',\n",
    "      type=int,\n",
    "      help='Rating weight type (0=linear, 1=log)',\n",
    "      default=LINEAR_RATINGS\n",
    "    )\n",
    "    parser.add_argument(\n",
    "      '--feature_wt_factor',\n",
    "      type=float,\n",
    "      help='Feature weight factor (linear ratings)',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "      '--feature_wt_exp',\n",
    "      type=float,\n",
    "      help='Feature weight exponent (log ratings)',\n",
    "    )\n",
    "\n",
    "    # other args\n",
    "    parser.add_argument(\n",
    "      '--output-dir',\n",
    "      help='GCS location to write model, overriding job-dir',\n",
    "    )\n",
    "    parser.add_argument(\n",
    "      '--verbose-logging',\n",
    "      default=False,\n",
    "      action='store_true',\n",
    "      help='Switch to turn on or off verbose logging and warnings'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "      '--hypertune',\n",
    "      default=False,\n",
    "      action='store_true',\n",
    "      help='Switch to turn on or off hyperparam tuning'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "      '--data-type',\n",
    "      type=str,\n",
    "      default='ratings',\n",
    "      help='Data type, one of ratings (e.g. MovieLens) or web_views (GA data)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "      '--delimiter',\n",
    "      type=str,\n",
    "      default='\\t',\n",
    "      help='Delimiter for csv data files'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "      '--headers',\n",
    "      default=False,\n",
    "      action='store_true',\n",
    "      help='Input file has a header row'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "      '--use-optimized',\n",
    "      default=False,\n",
    "      action='store_true',\n",
    "      help='Use optimized hyperparameters'\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    params = {\n",
    "    'weights': True,\n",
    "    'latent_factors': 5,\n",
    "    'num_iters': 20,\n",
    "    'regularization': 0.07,\n",
    "    'unobs_weight': 0.01,\n",
    "    'wt_type': 0,\n",
    "    'feature_wt_factor': 130.0,\n",
    "    'feature_wt_exp': 0.08,\n",
    "    'delimiter': '\\t',\n",
    "    'output_dir': 'output'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    params.update({k: arg for k, arg in arguments.items() if arg is not None})\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sh\n",
    "\n",
    "def save_model(args, user_map, item_map, row_factor, col_factor):\n",
    "\n",
    "    model_dir = os.path.join(args['output_dir'], 'model')\n",
    "    print(model_dir)\n",
    "\n",
    "    # if our output directory is a GCS bucket, write model files to /tmp,\n",
    "    # then copy to GCS\n",
    "    gs_model_dir = None\n",
    "    if model_dir.startswith('gs://'):\n",
    "        gs_model_dir = model_dir\n",
    "        model_dir = '/tmp/{0}'.format(args['job_name'])\n",
    "\n",
    "    os.makedirs(model_dir)\n",
    "    np.save(os.path.join(model_dir, 'user'), user_map)\n",
    "    np.save(os.path.join(model_dir, 'item'), item_map)\n",
    "    np.save(os.path.join(model_dir, 'row'), row_factor)\n",
    "    np.save(os.path.join(model_dir, 'col'), col_factor)\n",
    "\n",
    "    if gs_model_dir:\n",
    "        sh.gsutil('cp', '-r', os.path.join(model_dir, '*'), gs_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in divide\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/model\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    \n",
    "    args = parse_arguments()\n",
    "    \n",
    "    input_file = 'data/u.data'\n",
    "    \n",
    "    user_map, item_map, tr_sparse, test_sparse = create_test_and_train_sets(\n",
    "        args, input_file, args['data_type'])\n",
    "    \n",
    "    # train model\n",
    "    output_row, output_col = train_model(args, tr_sparse)\n",
    "    \n",
    "    save_model(args, user_map, item_map, output_row, output_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1083, 1239, 1141, 926, 49]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_rated = list(df[df.user_id==195].item_id)\n",
    "\n",
    "def generate_recommendations(user_idx, user_rated, row_factor, col_factor, k):\n",
    "    # bounds checking for args\n",
    "    assert (row_factor.shape[0] - len(user_rated)) >= k\n",
    "\n",
    "    # retrieve user factor\n",
    "    user_f = row_factor[user_idx]\n",
    "\n",
    "    # dot product of item factors with user factor gives predicted ratings\n",
    "    pred_ratings = col_factor.dot(user_f)\n",
    "\n",
    "    # find candidate recommended item indexes sorted by predicted rating\n",
    "    k_r = k + len(user_rated)\n",
    "    candidate_items = np.argsort(pred_ratings)[-k_r:]\n",
    "\n",
    "    # remove previously rated items and take top k\n",
    "    recommended_items = [i for i in candidate_items if i not in user_rated]\n",
    "    recommended_items = recommended_items[-k:]\n",
    "\n",
    "    # flip to sort highest rated first\n",
    "    recommended_items.reverse()\n",
    "\n",
    "    return recommended_items\n",
    "\n",
    "generate_recommendations(195, user_rated, output_row, output_col, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
